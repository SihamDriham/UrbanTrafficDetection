{
  "metadata": {
    "name": "Gradient Boosted Trees",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import col, sin, cos, when, sqrt, abs as spark_abs\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import GBTRegressor\nfrom pyspark.ml.evaluation import RegressionEvaluator\nimport numpy as np\nfrom pyspark.sql.functions import (\n    col, when, sin, cos\n)"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark\n# 1. Arrêter complètement la session existante\nspark.sparkContext.stop()\n\n# 2. Créer une nouvelle session avec les bonnes configurations\nspark \u003d SparkSession.builder \\\n    .appName(\"UrbanTrafficPrediction_GBT\") \\\n    .master(\"spark://spark-master:7077\") \\\n    .config(\"spark.sql.warehouse.dir\", \"hdfs://namenode:9000/user/hive/warehouse\") \\\n    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:9000\") \\\n    .enableHiveSupport() \\\n    .getOrCreate()\nspark.sparkContext.setLogLevel(\"ERROR\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark\n\n# Heures de pointe\ndf \u003d df.withColumn(\"is_peak_hour\", \n    when((col(\"Hour\").between(7, 9)) | (col(\"Hour\").between(16, 19)), 1).otherwise(0)\n)\n\n# Weekend\ndf \u003d df.withColumn(\"is_weekend\", \n    when(col(\"Day\").isin([5, 6]), 1).otherwise(0)\n)\n# Cycles temporels - Heure\ndf \u003d df.withColumn(\"hour_sin\", sin(2 * np.pi * col(\"Hour\") / 24))\ndf \u003d df.withColumn(\"hour_cos\", cos(2 * np.pi * col(\"Hour\") / 24))\ndf \u003d df.withColumn(\"hour_sin2\", sin(4 * np.pi * col(\"Hour\") / 24))\ndf \u003d df.withColumn(\"hour_cos2\", cos(4 * np.pi * col(\"Hour\") / 24))\n\n# Cycles temporels - Jour\ndf \u003d df.withColumn(\"day_sin\", sin(2 * np.pi * col(\"Day\") / 7))\ndf \u003d df.withColumn(\"day_cos\", cos(2 * np.pi * col(\"Day\") / 7))\n\n# Cycles temporels - Mois\ndf \u003d df.withColumn(\"month_sin\", sin(2 * np.pi * col(\"Month\") / 12))\ndf \u003d df.withColumn(\"month_cos\", cos(2 * np.pi * col(\"Month\") / 12))\n\n# Termes quadratiques\ndf \u003d df.withColumn(\"Hour_sq\", col(\"Hour\") ** 2)\ndf \u003d df.withColumn(\"temp_sq\", col(\"temp\") ** 2)\ndf \u003d df.withColumn(\"rain_sq\", col(\"rain_1h\") ** 2)\n\n# Interactions temporelles\ndf \u003d df.withColumn(\"hour_peak\", col(\"Hour\") * col(\"is_peak_hour\"))\ndf \u003d df.withColumn(\"hour_day\", col(\"Hour\") * col(\"Day\"))\ndf \u003d df.withColumn(\"hour_temp\", col(\"Hour\") * col(\"temp\"))\n\n# Interactions météo\ndf \u003d df.withColumn(\"temp_peak\", col(\"temp\") * col(\"is_peak_hour\"))\ndf \u003d df.withColumn(\"temp_rain\", col(\"temp\") * col(\"rain_1h\"))\ndf \u003d df.withColumn(\"temp_snow\", col(\"temp\") * col(\"snow_1h\"))\n\n# Interactions période\ndf \u003d df.withColumn(\"holiday_weekend\", col(\"holiday\") * col(\"is_weekend\"))\n\n# Conditions extrêmes\ndf \u003d df.withColumn(\"is_cold\", when(col(\"temp\") \u003c 0, 1).otherwise(0))\ndf \u003d df.withColumn(\"is_hot\", when(col(\"temp\") \u003e 25, 1).otherwise(0))\ndf \u003d df.withColumn(\"heavy_rain\", when(col(\"rain_1h\") \u003e 5, 1).otherwise(0))\n\n# Liste complète des features\nfeature_cols \u003d [\n    \"temp\", \"rain_1h\", \"snow_1h\",\n    \"Hour\", \"Day\", \"Month\", \"Year\",\n    \"holiday\", \"weather_description\",\n    \"is_peak_hour\", \"is_weekend\",\n    \"hour_sin\", \"hour_cos\", \"hour_sin2\", \"hour_cos2\",\n    \"day_sin\", \"day_cos\",\n    \"month_sin\", \"month_cos\",\n    \"Hour_sq\", \"temp_sq\", \"rain_sq\",\n    \"hour_peak\", \"hour_day\", \"hour_temp\",\n    \"temp_peak\", \"temp_rain\", \"temp_snow\",\n    \"holiday_weekend\",\n    \"is_cold\", \"is_hot\", \"heavy_rain\"\n]\n\nprint(f\"\\nNombre de features: {len(feature_cols)}\")"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark\n# Assembler toutes les features en un seul vecteur\nassembler \u003d VectorAssembler(\n    inputCols\u003dfeature_cols,\n    outputCol\u003d\"features\"\n)\n\ndf_assembled \u003d assembler.transform(df)\n\n# Sélectionner seulement les colonnes nécessaires\ndf_final \u003d df_assembled.select(\"features\", col(\"traffic_volume\").alias(\"label\"))\n\n# Split train/test (80/20)\ntrain_data, test_data \u003d df_final.randomSplit([0.8, 0.2], seed\u003d42)\n\nprint(f\"\\nTaille train: {train_data.count()}\")\nprint(f\"Taille test: {test_data.count()}\")"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark\n# Entraînement et prédiction avec GBTRegressor\ngbt \u003d GBTRegressor(\n    maxIter\u003d150,              \n    maxDepth\u003d7,\n    stepSize\u003d0.05,            \n    subsamplingRate\u003d0.8,   \n    minInstancesPerNode\u003d2,\n    maxBins\u003d64,\n    seed\u003d42,\n    featuresCol\u003d\"features\",\n    labelCol\u003d\"label\",\n    predictionCol\u003d\"prediction\"\n)\n\nprint(\"\\nEntraînement du modèle en cours...\")\nmodel \u003d gbt.fit(train_data)\n\n# Prédictions\npredictions \u003d model.transform(test_data)"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark\n#Évaluation des performances du modèle GBT\nprint(\"\\nÉchantillon de prédictions:\")\npredictions.select(\"label\", \"prediction\").show(10)\n# Évaluateur R²\nevaluator_r2 \u003d RegressionEvaluator(\n    labelCol\u003d\"label\",\n    predictionCol\u003d\"prediction\",\n    metricName\u003d\"r2\"\n)\n\n# Évaluateur RMSE\nevaluator_rmse \u003d RegressionEvaluator(\n    labelCol\u003d\"label\",\n    predictionCol\u003d\"prediction\",\n    metricName\u003d\"rmse\"\n)\n\n\n# Évaluateur MAE\nevaluator_mae \u003d RegressionEvaluator(\n    labelCol\u003d\"label\",\n    predictionCol\u003d\"prediction\",\n    metricName\u003d\"mae\"\n)\n\nr2 \u003d evaluator_r2.evaluate(predictions)\nrmse \u003d evaluator_rmse.evaluate(predictions)\nmae \u003d evaluator_mae.evaluate(predictions)\n\nprint(f\"\\n{\u0027\u003d\u0027*60}\")\nprint(f\"RÉSULTATS DU MODÈLE\")\nprint(f\"{\u0027\u003d\u0027*60}\")\nprint(f\"R² Score : {r2:.4f}\")\nprint(f\"RMSE     : {rmse:.2f}\")\nprint(f\"MAE      : {mae:.2f}\")\nprint(f\"{\u0027\u003d\u0027*60}\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark\n# Création de la base Hive et enregistrement des métriques du modèle\nspark.sql(\"CREATE DATABASE IF NOT EXISTS traffic_ml\")\nspark.sql(\"USE traffic_ml\")\nspark.sql(\"CREATE TABLE IF NOT EXISTS model_metrics (model_name STRING, rmse DOUBLE, r2 DOUBLE, mae DOUBLE)\")\n\nspark.sql(f\"\"\"\n    INSERT INTO model_metrics VALUES (\n        \u0027Gradient-Boosted Trees\u0027,\n        {rmse},\n        {r2},\n        {mae}\n    )\n\"\"\")\n# Affichage des métriques des modèles enregistrées\nspark.sql(\"SELECT * FROM traffic_ml.model_metrics\").show()"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark\n# Récupérer l\u0027importance des features\nfeature_importances \u003d model.featureImportances.toArray()\n\n# Créer un DataFrame pour l\u0027importance\nimport pandas as pd\nimportance_df \u003d pd.DataFrame({\n    \"Feature\": feature_cols,\n    \"Importance\": feature_importances\n}).sort_values(by\u003d\"Importance\", ascending\u003dFalse)\n\nprint(f\"\\nTop 15 features importantes:\")\nprint(importance_df.head(15))"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark\n"
    }
  ]
}