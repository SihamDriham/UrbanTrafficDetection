{
  "metadata": {
    "name": "Nettoyage des données",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark\n# Importation des bibliothèques\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import year, month, dayofmonth, hour, to_timestamp, col, when, median, expr, sum as Fsum\nfrom pyspark.sql.types import *\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pyspark.ml.feature import (\n    StringIndexer,\n    OneHotEncoder,\n    VectorAssembler,\n    StandardScaler,\n    MinMaxScaler\n)\n\nfrom pyspark.ml import Pipeline"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Création de la SparkSession\nspark \u003d SparkSession.builder \\\n    .appName(\"TrafficVolumeEDA\") \\\n    .getOrCreate()"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n\n%pyspark\n# Chargement du dataset\ntraffic_df \u003d spark.read \\\n    .option(\"header\", \"true\") \\\n    .option(\"inferSchema\", \"true\") \\\n    .csv(\"/data/raw/Metro_Interstate_Traffic_Volume.csv\")\n\ntraffic_df.printSchema()\ntraffic_df.show(5)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark\n# Sauvegarde des données au format Parquet dans HDFS\ntraffic_df.write \\\n    .mode(\"overwrite\") \\\n    .parquet(\"hdfs://namenode:9000/traffic_volume_parquet\")\n\ndf_hdfs \u003d spark.read.parquet(\"hdfs://namenode:9000/traffic_volume_parquet\")\ndf_hdfs.show(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark\ntraffic_df \u003d spark.read.parquet(\"hdfs://namenode:9000/traffic_volume_parquet\")\n\n# Afficher le schéma\ntraffic_df.printSchema()\n\n# Afficher quelques lignes\ntraffic_df.show(5)\n\n# Nombre de lignes\nprint(f\"Nombre de lignes : {traffic_df.count()}\")"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Statistiques de base\ntraffic_df.describe().show()"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Vérifier les nulls\ntraffic_df.select([Fsum(col(c).isNull().cast(\"int\")).alias(c) for c in traffic_df.columns]).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Conversion vers Pandas pour EDA\n# Colonnes numériques à analyser\nnum_cols \u003d [\u0027clouds_all\u0027,\u0027rain_1h\u0027,\u0027snow_1h\u0027,\u0027temp\u0027,\u0027traffic_volume\u0027]\neda_df \u003d traffic_df.select(num_cols).toPandas()"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n%matplotlib inline\n# Corrélation\ncorr \u003d eda_df.corr()\n\nplt.figure(figsize\u003d(10,4))\nsns.heatmap(corr, annot\u003dTrue, linewidths\u003d0.5, cmap\u003d\u0027twilight\u0027)\nplt.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n%matplotlib inline\n# Histogrammes\neda_df.hist(figsize\u003d(12,8))\nplt.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n%matplotlib inline\n#Analyse des valeurs aberrantes\nplt.figure(figsize\u003d(14,15))\n\nfor i, c in enumerate(num_cols, 1):\n    plt.subplot(3,2,i)\n    plt.boxplot(eda_df[c])\n    plt.title(c)\n\nplt.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Identification du maximum de pluie horaire\ntraffic_df.filter(col(\u0027rain_1h\u0027) \u003d\u003d traffic_df.agg({\"rain_1h\": \"max\"}).collect()[0][0]).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Filtrage des conditions de pluie très intense\nvery_heavy_rain_df \u003d traffic_df.filter(col(\u0027weather_description\u0027) \u003d\u003d \u0027very heavy rain\u0027).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Recherche du minimum de température\ntraffic_df.filter(col(\u0027temp\u0027) \u003d\u003d traffic_df.agg({\"temp\": \"min\"}).collect()[0][0]).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Calculer la médiane conditionnelle\nmedian_val \u003d traffic_df.filter((col(\u0027weather_description\u0027)\u003d\u003d\u0027very heavy rain\u0027) \u0026 (col(\u0027traffic_volume\u0027) \u003e 4000)) \\\n                        .approxQuantile(\"rain_1h\", [0.5], 0.01)[0]"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Remplacer les valeurs maximales par la médiane\nmax_rain \u003d traffic_df.agg({\"rain_1h\": \"max\"}).collect()[0][0]\ntraffic_df \u003d traffic_df.withColumn(\"rain_1h\",\n                                   when(col(\"rain_1h\") \u003d\u003d max_rain, median_val)\n                                   .otherwise(col(\"rain_1h\")))"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Convertir date_time en timestamp\ntraffic_df \u003d traffic_df.withColumn(\"date_time\", to_timestamp(col(\"date_time\"), \"yyyy-MM-dd HH:mm:ss\"))"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Extraire Year, Month, Day, Hour\ntraffic_df \u003d traffic_df.withColumn(\"Year\", year(col(\"date_time\"))) \\\n                       .withColumn(\"Month\", month(col(\"date_time\"))) \\\n                       .withColumn(\"Day\", dayofmonth(col(\"date_time\"))) \\\n                       .withColumn(\"Hour\", hour(col(\"date_time\")))"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Nettoyage des données de température\ntraffic_df \u003d traffic_df.withColumn(\"temp\",\n                                   when((col(\"Year\")\u003d\u003d2014) \u0026 (col(\"Month\")\u003d\u003d1) \u0026 (col(\"Day\")\u003d\u003d31) \u0026 (col(\"temp\")\u003d\u003d0.0),\n                                        255.93)\n                                   .otherwise(col(\"temp\")))"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n#Sélection des colonnes pertinentes\ntraffic_df \u003d traffic_df.select(\n    \u0027holiday\u0027,\u0027temp\u0027,\u0027rain_1h\u0027,\u0027snow_1h\u0027,\u0027Year\u0027,\u0027Month\u0027,\u0027Day\u0027,\u0027Hour\u0027,\n    \u0027weather_main\u0027,\u0027weather_description\u0027,\u0027traffic_volume\u0027\n)"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n#Sauvegarde des données nettoyées dans HDFS\ntraffic_df.write.mode(\"overwrite\").parquet(\"hdfs://namenode:9000/traffic_volume_cleaned\")"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n#Lecture des données nettoyées depuis HDFS\ntraffic_df \u003d spark.read.parquet(\"hdfs://namenode:9000/traffic_volume_cleaned\")"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# 1️⃣ Créer les indexers avec outputCol temporaire\nindexers \u003d [\n    StringIndexer(inputCol\u003d\"holiday\", outputCol\u003d\"holiday_tmp\", handleInvalid\u003d\"keep\"),\n    StringIndexer(inputCol\u003d\"weather_main\", outputCol\u003d\"weather_main_tmp\", handleInvalid\u003d\"keep\"),\n    StringIndexer(inputCol\u003d\"weather_description\", outputCol\u003d\"weather_description_tmp\", handleInvalid\u003d\"keep\")\n]\n\n# 2️⃣ Créer le pipeline\npipeline \u003d Pipeline(stages\u003dindexers)\n\n# 3️⃣ Appliquer le pipeline\ntraffic_df \u003d pipeline.fit(traffic_df).transform(traffic_df)\n\n# 4️⃣ Supprimer les colonnes originales et renommer les colonnes encodées pour garder les mêmes noms\ntraffic_df \u003d traffic_df.drop(\"holiday\", \"weather_main\", \"weather_description\") \\\n                       .withColumnRenamed(\"holiday_tmp\", \"holiday\") \\\n                       .withColumnRenamed(\"weather_main_tmp\", \"weather_main\") \\\n                       .withColumnRenamed(\"weather_description_tmp\", \"weather_description\")\n\n# 5️⃣ Vérifier le résultat\ntraffic_df.select(\"holiday\", \"weather_main\", \"weather_description\").show(10, truncate\u003dFalse)\n\n\n# 5️⃣ Écrire en Parquet\ntraffic_df.write.mode(\"overwrite\").parquet(\n    \"hdfs://namenode:9000/traffic_volume_cleaned_encoded.parquet\"\n)\n\n# 6️⃣ Lire plus tard (optionnel)\nencoded_df \u003d spark.read.parquet(\n    \"hdfs://namenode:9000/traffic_volume_cleaned_encoded.parquet\"\n)\nencoded_df.show(5)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}