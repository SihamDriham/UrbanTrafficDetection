{
  "metadata": {
    "name": "DecisionTree",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import DecisionTreeRegressor\nfrom pyspark.ml.evaluation import RegressionEvaluator"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# 1. Arrêter complètement la session existante\nspark.sparkContext.stop()\n\n# 2. Créer une nouvelle session avec les bonnes configurations\nspark \u003d SparkSession.builder \\\n    .appName(\"TrafficVolume_DecisionTree\") \\\n    .master(\"spark://spark-master:7077\") \\\n    .config(\"spark.sql.warehouse.dir\", \"hdfs://namenode:9000/user/hive/warehouse\") \\\n    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:9000\") \\\n    .enableHiveSupport() \\\n    .getOrCreate()"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark\n#Lecture des données nettoyées depuis HDFS\ntraffic_df \u003d spark.read.parquet(\"hdfs://namenode:9000/traffic_volume_cleaned_encoded.parquet\")"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n\n%pyspark\n#Préparation des données et split Train/Test\n\ntraffic_df \u003d traffic_df.withColumnRenamed(\n    \"traffic_volume\", \"label\"\n)\n\ntrain_df, test_df \u003d traffic_df.randomSplit(\n    [0.75, 0.25], seed\u003d0\n)\n\nprint(\"Train size:\", train_df.count())\nprint(\"Test size :\", test_df.count())\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n\n%pyspark\n\n# 1️⃣ Colonnes features uniquement numériques / encodées\nfeature_cols \u003d [\n    \"temp\", \"rain_1h\", \"snow_1h\",\n    \"Year\", \"Month\", \"Day\", \"Hour\",\n    \"holiday\", \"weather_main\", \"weather_description\"  # supposons que ce sont les colonnes encodées\n]\n\n# 2️⃣ Créer VectorAssembler\nassembler \u003d VectorAssembler(\n    inputCols\u003dfeature_cols,\n    outputCol\u003d\"features\"\n)\n\n# 3️⃣ Transformer train/test\ntrain_df, test_df \u003d traffic_df.randomSplit([0.75, 0.25], seed\u003d0)\n\ntrain_df \u003d assembler.transform(train_df)\ntest_df  \u003d assembler.transform(test_df)\n\n# 4️⃣ Vérifier\ntrain_df.select(\"features\", \"label\").show(5)\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\n%pyspark\n\n# 1️⃣ Créer le modèle Spark ML\ndt \u003d DecisionTreeRegressor(featuresCol\u003d\"features\", labelCol\u003d\"label\", maxBins\u003d50)\n\n# 2️⃣ Entraîner le modèle\ndt_model \u003d dt.fit(train_df)  # train_df contient \u0027features\u0027 et \u0027label\u0027\n\n# 3️⃣ Faire les prédictions sur test set\npredictions \u003d dt_model.transform(test_df)\npredictions.select(\"label\", \"prediction\").show(10)\n\n# 4️⃣ Évaluer le modèle\nevaluator_rmse \u003d RegressionEvaluator(\n    labelCol\u003d\"label\", predictionCol\u003d\"prediction\", metricName\u003d\"rmse\"\n)\nevaluator_r2 \u003d RegressionEvaluator(\n    labelCol\u003d\"label\", predictionCol\u003d\"prediction\", metricName\u003d\"r2\"\n)\nevaluator_mae \u003d RegressionEvaluator(\n    labelCol\u003d\"label\", predictionCol\u003d\"prediction\", metricName\u003d\"mae\"\n)\n\nrmse \u003d evaluator_rmse.evaluate(predictions)\nr2 \u003d evaluator_r2.evaluate(predictions)\nmae \u003d evaluator_mae.evaluate(predictions)\n\nprint(\"R Squared (R²)        :\", r2)\nprint(\"Mean Absolute Error    :\", mae)\nprint(\"Root Mean Squared Error:\", rmse)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n#Création de la base Hive et enregistrement des métriques du modèle\n\nspark.sql(\"CREATE DATABASE IF NOT EXISTS traffic_ml\")\nspark.sql(\"USE traffic_ml\")\nspark.sql(\"CREATE TABLE IF NOT EXISTS model_metrics (model_name STRING, rmse DOUBLE, r2 DOUBLE, mae DOUBLE)\")\n\nspark.sql(f\"\"\"\n    INSERT INTO model_metrics VALUES (\n        \u0027DecisionTree\u0027,\n        {rmse},\n        {r2},\n        {mae}\n    )\n\"\"\")"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n#Affichage des métriques des modèles enregistrées\nspark.sql(\"SELECT * FROM traffic_ml.model_metrics\").show()"
    }
  ]
}